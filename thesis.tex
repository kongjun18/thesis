% !TEX TS-program = xelatex
% !TEX encoding = UTF-8 Unicode

% \documentclass[AutoFakeBold]{LZUThesis}
\documentclass[AutoFakeBold]{LZUThesis}
\usepackage[inkscapelatex=false]{svg}
\newcommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
  \usepackage{shellesc}

\begin{document}
\begin{sloppypar}
%=====%
%
%封皮页填写内容
%
%=====%

% 标题样式 使用 \title{{}}; 使用时必须保证至少两个外侧括号
%  如： 短标题 \title{{第一行}},
% 	      长标题 \title{{第一行}{第二行}}
%             超长标题\tiitle{{第一行}{...}{第N行}}

\title{{基于 RISC-V 指令集的教学操作系统内核}{内存分配器设计与实现}}


% 标题样式 使用 \entitle{{}}; 使用时必须保证至少两个外侧括号
%  如： 短标题 \entitle{{First row}},
% 	      长标题 \entitle{{First row}{ Second row}}
%             超长标题\entitle{{First row}{...}{ Next N row}}
% 注意：  英文标题多行时 需要在开头加个空格 防止摘要标题处英语单词粘连。
\entitle{{Design and Implementation of Memory Allocator}{for Teaching Operating System Based on RISC-V}}

\author{孔俊}
\major{计算机科学与技术}
\advisor{李守亮}
\college{信息科学与工程学院}
\grade{2019级}



\maketitle



%==============================%
% ↓ ↓ ↓ 诚信说明页 授权说明书
%==============================%

% 1. 可以调整签字的宽度，现在是40
% 2. 去掉raisebox的相关注释(注意上下大括号对应)，可以改变-5那个数字调整签名和横线的上下位置

% 你的签名，signature.pdf 改为你的签名文件名，
\mysignature{
    \raisebox{-5pt}{
        \includegraphics[width=40pt]{images/my-signature.pdf}
    }
}
% % 你手写的日期，signature.pdf 改为你的手写的日期文件名
% \mytime{
%     % \raisebox{-5pt}{
%         \includegraphics[width=40pt]{images/my-time.pdf}
%     % }
% }
% % 老师的手写签名，signature.pdf 改为老师的手写签名文件名
% \supervisorsignature{
%     % \raisebox{-5pt}{
%         \includegraphics[width=40pt]{signature.pdf}
%     % }
% }
% % 老师手写的时间，signature.pdf 改为老师的手写的日期文件名
% \teachertime{
%     % \raisebox{-5pSt}{
%         \includegraphics[width=40pt]{signature.pdf}
%     % }
% }
% % 老师手写的成绩
% \recommendedgrade{
%     % \raisebox{-5pt}{
%         \includegraphics[width=40pt]{signature.pdf}
%     % }
% }

\makestatement

%==============================%
% ↑ ↑ ↑ 诚信说明页 授权说明书
%==============================%


%=====%
%论文（设计）成绩：注意2007的模板要求，成绩页在最后，2021要求成绩页在摘要前面
%=====%

% % 下面这些注释掉可以去掉成绩、评语什么的
% \supervisorcomment{}


% \committeecomment{}

% \finalgrade{}
% % 上面这些注释掉可以去掉成绩、评语什么的


\frontmatter


%中文摘要
\ZhAbstract{
在计算机学科的教学中，工程实践是必不可少的一环。教学操作系统是适用于操作系
统课程实践教学的简易操作系统。物理内存管理子系统是操作系统最基础的子系统，其核心是内存分配器，被系统其他所有模块依赖。

本论文概述了 Linux 内核的物理内存管理子系统的发展现状，分析了 Linux 内核的内存模型演化进程。本论文完成了基于 RISC-V 指令集的教学操作系统中物理内存管理子系统的设计与实现，包括物理内存探测、页分配器、slab 分配器等。此外，还分析了内存分配器内存碎片情况与改进方法。
}{操作系统；内存分配器；内存管理；伙伴系统；硬件缓存}

%英文摘要
\EnAbstract {In the teaching of computer science, engineering practice is an essential part. Teaching Operating Systems is applicable to the Department of Operating Systems
A simple operating system for practical teaching of systemic courses. The physical memory management subsystem is the most basic subsystem of the operating system. Its core is the memory allocator, which is relied upon by all other modules of the system.

This paper summarizes the development status of the physical memory management subsystem of the Linux kernel, and analyzes the evolution process of the memory model of the Linux kernel. This thesis completes the design and implementation of the physical memory management subsystem in the teaching operating system based on the RISC-V instruction set, including physical memory detection, page allocator, slab allocator, etc. In addition, the memory fragmentation of the memory allocator and the improvement methods are also analyzed.
}
{
operating system; memory management; buddy system; cache
}



%生成目录
% \tableofcontents
% 下面这个包含图表目录
\customcontent


% % 部分同学需要专业术语注释表，* 表示不加入目录
% \chapter*{专业术语注释表}
% \begin{longtable}{lll}
%   \caption*{缩略词说明}\\
%   SS & Spread Spectrum & 扩展频谱 \\
%   PAPR & Peak to Average Power Ratio & 峰均比\\
%   DCSK & Differential Chaos Shift Keying &差分混移位键控\\
%   dasd & fdhfudw eqwrqw fasfasfs fewev wqfwefew &\tabincell{l}{太长了\\换行一下}\\
% \end{longtable}


%文章主体
\mainmatter

\chapter{绪论}


\section{背景与意义}

操作系统作为一门计算机类专业的专业基础课程，在培养学生的逻辑思维能力、问题分析和解决能力以及计算机系统能力等方面具有重要的地位。而实验与实践训练是帮助学生理解课程知识的一个重要而有效的环节，设计良好的操作系统课程实验环节有助于学生深入理解操作系统的各个方面，培养学生系统分析与设计的能力。

随着操作系统理论的不断发展和完善，以及操作系统实际需求的不断变更迭代，各大操作系统均不同程度的引入了各种先进的算法实现和优秀的架构设计，对这些算法和架构的学习有利于加深对操作系统底层原理的理解，加强动手实践能力。而这就要求在实验教学过程中必须深入到操作系统的具体实现细节中，通过理论与实践结合的方式进行教学。

内存管理子系统是操作系统最重要最基础的子系统。以 Linux
内核为例，目前内存管理子系统是 Linux
内核最复杂的顶级子系统，以最新（截止 2023 年 5 月）的 Linux v6.2
版本为例，Linux 源代码目录 mm 共计 176 个文件、182932
余行代码（包括注释和空行），而作为云计算基石的 KVM（Kernel-based Virtual
Machine）源代码仅有 9396 余行。

KVM 和内存管理子系统的代码量统计表见表\ref{table:kvm-code-stastic}和表\ref{table:mm-code-stastic}。

\begin{longtable}[]{@{}llllll@{}}
\toprule\noalign{}
Language & Files & Lines & Code & Comments & Blanks \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
C & 10 & 9271 & 6451 & 1273 & 1547 \\
C Header & 4 & 121 & 75 & 26 & 20 \\
Makefile & 2 & 4 & 2 & 2 & 0 \\
Total & 16 & 9396 & 6528 & 1301 & 1567 \\
\label{table:kvm-code-stastic}
\caption{KVM 代码统计}
\end{longtable}

内存子系统代码统计如下：

\begin{longtable}[]{@{}llllll@{}}
\toprule\noalign{}
Language & Files & Lines & Code & Comments & Blanks \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
C & 151 & 178276 & 113360 & 40752 & 24164 \\
C Header & 20 & 4424 & 2869 & 860 & 695 \\
Makefile & 5 & 232 & 178 & 26 & 28 \\
Total & 176 & 182932 & 116407 & 41638 & 24887 \\
\label{table:mm-code-stastic}
\caption{内存管理子系统代码统计}
\end{longtable}

Linux
内核的源代码过于复杂，难以用于教学，而目前的许多教学操作系统虽然实现了物理内存的管理，但是其设计和实现过于简单。本论文聚焦于教学操作系统的物理内存管理模块的设计，使用伙伴系统加
slab 分配器的经典设计，完整实现了页分配器和 slab
分配器，可以用于操作系统教学。

物理内存管理的目的是进行高效的物理内存分配与释放，从这个角度看，物理内存管理子系统就等同于内核物理内存分配器。因此，在本论文中，''物理内存管理子系统``等同于''物理内存分配器``。


\section{现代操作系统内存分配器概述}

Linux
是目前市场占有率最高、影响力最大的开源操作系统内核，而且是云计算的基石。因此，笔者以
Linux 内核为例，概述现代操作系统内存分配器的发展情况。

内存管理子系统是操作系统内核最核心、最底层的子系统，被其余所有子系统依赖。因此，优秀的内存分配器必须做到以下几点：

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  低内存碎片
\end{enumerate}

内存碎片会导致系统内存资源利用率下降，甚至导致系统无法分配内存。

对于内存分配负载大的工作流（如游戏服务器）以及长期运行的系统（如移动设备），低内存碎片尤为重要。Linux
内核通过页面合并、内存重整、伙伴系统和 slab
分配器结合等技术降低内存碎片。

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  低延迟
\end{enumerate}

内存分配器应该尽可能地快速响应内存分配请求，以减少进程等待时间和系统负载。

低延迟的内存分配对于中断处理例程尤为重要。以接收网络包为例，网卡接收到数据包后，会向
CPU 发送中断信号，陷入到中断处理程序（interrupt
handler）。中断处理程序必须尽快响应网卡的中断请求，并将数据包从网卡的接收缓冲区复制到内核的网络接收缓冲区（socket
buffer）中，完成复制后才可以响应下一次网卡的中断请求。如果内存分配器的延迟过大甚至无可用内存，会导致中断处理程序失败或等待可用内存，极大降低系统吞吐量。

为此，Linux
内存分配器会系统保留一部分内存作为紧急内存池，用于系统内存吃紧时兜底。并通过
slab 分配器、内存池缓存生命周期短且可以循环利用的对象，以降低分配延迟。

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  可扩展性
\end{enumerate}

内存分配器应该能够处理不同规模的内存分配请求，能够适应系统负载的变化。

内存分配器既要能高效地支持小块内存分配，也应该支持大块内存分配，并且不局限于特定大小。例如，使用分离链表算法的内存分配器，会缓存特定大小的内存块，请求这些特定大小的内存块是时，响应时间极短，但对于其他大小的内存请求响应时间更长。

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  内存安全
\end{enumerate}

内存分配器应该能够确保内存分配和释放的安全性，避免内存泄漏和内存越界等问题。

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  可定制性
\end{enumerate}

内存分配器应该允许用户自定义内存分配策略，以适应特定的应用场景和系统需求。

例如在 x86 架构下，只有物理地址低于 32M 的物理内存可以用于
DMA，内存分配器必须能支持分配物理地址低于 32M 的物理内存。

除了这些传统的目标外，随着硬件技术的进步，目前操作系统内存分配器必须支持
NUMA 架构、内存热插拔、多核处理器等。

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  NUMA 架构
\end{enumerate}

\emph{NUMA}（Non-uniform memory access) 即
\emph{非一致内存访问}，在高端服务器领域越来越流行。

在一个 NUMA 架构中，CPU 和内存被划分为多个节点，每个节点通常有多个 CPU
和多个内存控制器，节点通过高速互联网络连接在一起。每个 CPU
都有本地内存和远程内存，本地内存直接连接到本地
CPU，而远程内存需要通过互联网络连接到远程
CPU。因此访问本地内存的延迟比访问远程内存的延迟要小。因此，在 NUMA
架构中，需要对内存进行合理的分配，以充分利用本地内存，并尽量减少访问远程内存的次数，以提高系统的性能和效率。

NUMA 架构下不仅 CPU
访问本地内存和远程内存延迟不同，而且内存地址空间也变得更加稀疏和不连续，操作系统内核内存分配器必须应对这一挑战。

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  内存热插拔
\end{enumerate}

内存热插拔技术是一种可以在服务器运行时添加或删除物理内存的技术。使用内存热插拔技术在不停机的情况下进行增删物理内存是云计算时代下的基本需求。

内存热插拔技术在云计算中至少有以下典型应用场景：

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  虚拟机内存扩容：云计算中的虚拟机通常会运行多个不同的应用程序，这些应用程序的内存需求可能随时变化。通过内存热插拔技术，虚拟机可以在运行状态下扩展内存，以满足应用程序的内存需求。
\item
  虚拟机故障修复：虚拟机中的内存模块可能会出现故障，导致虚拟机无法正常运行。通过内存热插拔技术，可以在不影响虚拟机运行的情况下，快速替换故障的内存模块，恢复虚拟机的正常运行。
\end{enumerate}

内存热插拔不仅需要硬件支持，更需要操作系统支持。内存管理子系统必须能够检测物理内存的插入和拔出，在插入时检测物理内存并将其纳入到内存子系统的管理之下，拔出内存时必时必须将内存的将内存中的数据迁移到系统的其他物理内存中，避免内存错误。

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  多核处理器
\end{enumerate}

在多核系统中，多个 CPU
可能同时请求内存，如果内存分配器不能很好地扩展，就会出现竞争和互斥等问题，从而导致性能瓶颈，甚至性能远低于单核系统。

为了提高内存分配器的可扩展性，Linux 内存子系统大量使用本地 CPU
缓存。Linux 内核中的几乎所有内存分配器都为每个 CPU
分配自己的本地缓存，优先从本地缓存分配内了存，以避免线程间同步。

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  内存安全与调试
\end{enumerate}

通常，内核检测的内存安全 bug 可以分为以下几类：

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  OOB(Out Of Bound)：内存访问越过了已分配内存的边界。
\item
  UAF(Use After Free)：访问已释放的内存。
\item
  Invalid Free: 无效释放，即内存释放函数（如 C
  标准库的\texttt{free()}）释放的内存不是之前分配的内存。
\item
  Double Free：二次释放，即释放内存后又通过内存释放函数释放内存。
\end{enumerate}

内存安全 bug
会对内核稳定性产生负面影响。\href{https://source.android.google.cn/docs/security/memory-safety?hl=zh-cn}{AOSP
安全性文档}指出，内存安全 bug 和以原生编程语言处理内存时遇到的错误是
Android 代码库中最常见的问题。此类问题造成了超过 60\%
的高严重程度安全漏洞，并造成了数百万次用户可见的崩溃。从第一个 Android
版本开始，内存安全 bug 就一直是 Android 安全漏洞的首要原因，约占 Android
安全漏洞的 51\%。

\begin{figure}
\centering
\includegraphics[width=300pt]{images/Android-security-report.png}
\caption{Android 安全漏洞类型}
\end{figure}

随着代码的复杂性不断增长，如果一直坐视不管，内存安全 bug
的数量将随着时间的推移而持续增加。因此，现代操作系统必须提供检测并缓解此类
bug 的工具。

Linux 内核于 6.1 版本开始支持使用 Rust
语言编写驱动程序，试图通过引入内存安全型编程语言环境内存安全问题。此外，从
Linux 2.x 版本开始，Linux 内核就将内存安全 bug
检测作为内存管理子系统的重要功能。目前，Linux
内核中的几乎所有类型的内存分配都有对应的内存安全 bug
检测器，主要分为以下三种：

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  内存分配器内置的调试功能：内存分配器在分配内存和释放时，检测是否发生内存安全
  bug。这种方式无法在发生内存安全 bug 时立刻报告错误，但性能损耗比较小。
\item
  KASAN(The Kernel Address Sanitizer)：使用 shadow memory
  记录所有内存的使用情况（正常/异常），即使用 1 字节记录 8
  个字节的使用情况；通过编译器的协助，在内核代码的每一条内存访问指令前插入
  KASAN 检测指令，检测是否出现内存安全 bug。KASAN
  在每条内存访问指令前插入 KASAN 检测指令，导致严重的性能损耗；KASAN
  使用 1 字节记录 8 个字节的使用情况，因此会占用系统总内存的 1/8。尽管
  KASAN 不能用于生产环境，但可以及时报告内存错误，是 Linux
  内核中功能最强的内存安全 bug 检测器。
\item
  KFENCE(Kernel Electric-Fence)：在 Linux 5.12 版本引入，仅检测通过
  KFENCE 内存池分配的内存（kfence 对象），而且是周期性采样。KFENCE
  性能开销比较低，可以用于生产环境，因此 Linux 5.12 版本后默认开启
  KFENCE。
\end{enumerate}

Linux 内核的三大类内存安全 bug 检测器比较表见表\ref{table:Linux-memory-security-bug-detector}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1237}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1340}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1856}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1856}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1856}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1856}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Debugger
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Overhead
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
OOB
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
UAF
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
invalid free
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
double free
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
内存分配器内置的调试功能 & 中等 & 对象被释放时 & 对象被重新分配时 & 立即
& 立即 \\
kasan & 非常高，不可用于生产环境。 & 立即 &
立即（对象被重新分配后不再检测） & 立即 & 立即 \\
kfence & 低，可用于生产环境。 & 立即或当 kfence 对象被释放时 &
立即（仅适用于 kfence 对象） & 立即（仅适用于 kfence 对象） &
立即（仅适用于 kfence 对象） \\
\label{table:Linux-memory-security-bug-detector}
\caption{Linux 内存安全 bug 检测器比较}
\end{longtable}


\section{论文主要工作与组织结构}

第一章：绪论。首先阐述了论文选题的背景和意义，随后概述了 Linux
内核内存管理子系统的最新进展，最后介绍本论文的主要工作与组织结构。

第二章：内存模型。首先概述 Linux
内核的内存模型演化，然后介绍本系统使用的内存模型与相关数据结构。

第三章：页分配器。本章描述如何跟踪物理页分配情况，然后介绍页分配器提供的
API 及其实现，最后分析内存碎片情况。

第四章：slab 分配器。本章首先概述 slab 分配器的设计原则，随后介绍 slab
分配器提供的 API 及其实现，最后分析硬件缓存利用情况和内存碎片。

第五章：结论与展望。简要概述系统的总体实现结果，展望未来本操作系统内存分配器的发展。



\chapter{内存模型}

\emph{内存模型}（\emph{memory
model}）指操作系统对物理内存的管理方式和组织结构。

Linux
内核的内存模型经历了三个阶段：平坦内存模型、非连续内存模型和稀疏内存模型。


\section{平坦内存模型}

在大多数处理器架构中，物理内存仅仅是物理地址空间的一部分，物理地址空间通常是不连续的，其中不可用的部分称为\emph{内存空洞}（\emph{memory
hole}）。内存空洞包括：架构保留的区域，内存映射 IO，ROM 等等。

x86\_64 典型的物理地址空间如图\ref{figure:x86_64-physical-address-space}。

\begin{figure}
\centering
\includesvg{images/x86_64-physical-address-space-latex.svg}
\caption{典型的 x86\_64 物理地址空间}
\label{figure:x86_64-physical-address-space}
\end{figure}

x86\_64 架构中，前 1MB
物理地址空间称为\emph{实模式地址空间}，以实模式地址空间为例，说明物理地址空间的复杂性：

\begin{table}[!ht]
\resizebox{\textwidth}{!}{
\centering
\begin{tabular}{|l|l|l|l|l|}
\hline
start
& end
& size
& description
& type
\\ \hline
Real mode address space (the first MiB)
& ~ & ~ & ~ & ~ \\ \hline
0x00000000
& 0x000003FF
& 1 KiB
& Real Mode IVT (Interrupt Vector Table)
& unusable in real mode
\\ \hline
0x00000400
& 0x000004FF
& 256 bytes
& BDA (BIOS data area)
& ~ \\ \hline
0x00000500
& 0x00007BFF
& almost 30 KiB
& Conventional memory
& usable memory
\\ \hline
0x00007C00
& 0x00007DFF
& 512 bytes
& Your OS BootSector
& ~ \\ \hline
0x00007E00
& 0x0007FFFF
& 480.5 KiB
& Conventional memory
& ~ \\ \hline
0x00080000
& 0x0009FFFF
& 128 KiB
& EBDA (Extended BIOS Data Area)
& partially used by the EBDA
\\ \hline
0x000A0000
& 0x000BFFFF
& 128 KiB
& Video display memory
& hardware mapped
\\ \hline
0x000C0000
& 0x000C7FFF
& 32 KiB (typically)
& Video BIOS
& ROM and hardware mapped / Shadow RAM
\\ \hline
0x000C8000
& 0x000EFFFF
& 160 KiB (typically)
& BIOS Expansions
& ~ \\ \hline
0x000F0000
& 0x000FFFFF
& 64 KiB
& Motherboard BIOS
& ~ \\ \hline
\end{tabular}
}
\end{table}

尽管物理地址空间如此复杂，但总体上内存仍然是连续的。平坦内存模型将物理地址空间视作由连续的物理页组成的数组，并使用数组\texttt{struct\ page\ mem\_map{[}{]}}跟踪每一页的状态，包括是否已分配等等。

\begin{figure}
\centering
\includesvg{images/FLATMEM.drawio.svg}
\caption{FLATMEM 内存模型}
\end{figure}

平坦内存模型有以下优点：

\begin{itemize}
\item
  符合直觉，实现简单。
\item
  快速进行\texttt{struct\ page*}和物理页号\texttt{pfn}的转换。
\end{itemize}


\section{非连续内存模型}

二十一世纪初，NUMA
架构的逐渐普及，导致物理内存更加稀疏，平坦内存模型的问题暴露出来：

\begin{itemize}
\item
  无法应对大量内存空洞：平坦内存模型使用全局数组\texttt{mem\_map{[}{]}}追踪每个物理页的状态，即使某些页帧对应的物理地址没有实际的内存，Linux
  也要为其分配\texttt{struct\ page}结构体，存在大量内存空洞意味着\texttt{mem\_map{[}{]}}中相当一部分\texttt{struct\ page}被浪费。
\item
  不支持 NUMA 架构：NUMA
  架构将内存划分为多个节点，而平坦内存模型却将物理内存视作一个连续的物理页数组。
\end{itemize}

为了克服平坦内存模型的上述问题，1999 年开始 Linux
引入了非连续内存模型以支持 NUMA 架构，该内存模型称为 DISCONTIGMEM。

在非连续内存模型中，内存被划分为多个节点，每个节点都有自己的\texttt{mem\_map{[}{]}}数组，跟踪本节点内的物理页。

\begin{figure}
\centering
\includesvg{images/DISTCONFIT.drawio.svg}
\caption{DISTCONFITMEM 内存模型}
\end{figure}

尽管非连续内存模型是为了支持 NUMA
架构而发生的，但非连续物理内存模型是操作系统的软件视角，不一定准确反映硬件
NUMA
布局。例如，操作系统可以把任意一块连续内存视作一个节点，而不用考虑硬件上
NUMA 架构的真实布局。论文后续部分会详细介绍。


\section{稀疏内存模型}

物理内存热插拔技术的进步，支持物理内存热插拔成为了对内核内存管理子系统的基本要求。在云计算场景下，往往需要通过内存热插拔可以用来实现动态扩容，但频繁的内存插拔也使得物理内存更加不连续。

内存热插拔技术使得大范围的连续物理内存不再场景，稀疏的物理内存称为常态。非连续内存模型尽管使得
Linux 成功支持 NUMA
架构，但非连续内存模型实际上只是连续物理内存的变体，每个节点内部都是连续内存模型。内存热插拔场景下的物理内存过于稀疏，粗粒度的非连续内存模型无法支持如此稀疏的物理内存布局。

因此，Linux 于 2005 年引入了原始的稀疏内存模型，称为
SPARSEMEM。SPARSEMEM
模型把粗粒度的节点，变成了更细粒度的\texttt{struct\ mem\_section}。每个\texttt{mem\_seciton}都有自己的内存管理结构\texttt{mem\_map{[}{]}}，一个\texttt{mem\_section}管理\(2^{SECTION\_SIZE\_BIT}\)字节物理内存，通常\texttt{SECTION\_SIZE\_BIT}
定义为 27，即一个\texttt{mem\_section} 管理 128MB
内存。整个系统的内存管理结构被视作一个\texttt{mem\_section}数组\footnote{实际上是一个
  NR\_SECTION\_ROOTS * SECTION\_PER\_ROOT 大小的二维数组，因为
  SECTION\_PER\_ROOT 定义为 1，因此这里说是一维数组。}。

将 FLATMEM 模型中 \texttt{struct\ page}
必须从物理地址开始到结束而连续存在，变成了 \texttt{struct\ mem\_section}
必须连续存在。在内存空洞的场景下，只需要每 128 MB 的物理地址空间存在一个
\texttt{struct\ mem\_section}
即可（不要求分配\texttt{struct\ page}结构），而无需为每 4 KB
的物理地址空间都分配一个
\texttt{struct\ page}，减少了不必要的内存开销。而且，\texttt{mem\_section}有上线和下线两个状态，对应内存的插入与拔出，通过
\texttt{struct\ mem\_section} 的动态初始化与销毁实现物理内存热插拔。

\begin{figure}
\centering
\includesvg[width=500pt]{images/SPARSEMEM.drawio.svg}
\caption{SPARSEMEM 内存模型}
\end{figure}

经典 SPARSEMEM 模型有两大缺点：

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \texttt{mem\_section{[}{]}}数组是固定分配的，而且覆盖整个物理地址空间，这会大量浪费内存。64
  位处理器的地址空间极大，64 位 RISCV
  架构支持\(2^{56}\)字节地址空间，x86\_64
  架构支持\(2^{52}\)字节或\(2^{46}\)字节物理地址空间，要覆盖 RISCV
  架构的\(2^{56}\)字节物理地址空间，需要分配\(2^{29} = 536,870,912\)个\texttt{mem\_section}结构体。
\item
  页帧编号和\texttt{struct\ page*}之间的转换开销比平坦内存模型大。
\end{enumerate}

这两个问题阻碍着 SPARSEMEM 完全替代 DISCONTIGMEM
内存模型，这两个问题已经分别通过 2005 年引入的 SPARSEMEM-EXTREME 拓展和
2007 年引入的 SPARSEMEM-VMEMMAP 拓展解决了。

SPARSEMEM-EXTREME 将经典 SPARSEMEM
内存模型的一维\texttt{mem\_map{[}{]}}改成动态分配的\texttt{mem\_section{[}NR\_SECTION\_ROOTS{]}{[}SECTIONS\_PER\_ROOT{]}}，当某个\texttt{mem\_seciton{[}SECTION\_PER\_ROOT{]}}对应的地址空间是内存空洞时，就不需要为其分配\texttt{struct\ mem\_seciton}数组，从而减少了内存消耗。

\begin{figure}
\centering
\includesvg[width=500pt]{images/SPARSEMEM-EXTREME.drawio.svg}
\caption{SPARSEMEM-EXTREME 内存模型}
\end{figure}

SPARSEMEM-VMEMMAP 拓展的思路是：在 SPARSEMEM 中，\texttt{struct\ page}
为应对内存空洞，实际上不会连续存在，但可以设法安排每个
\texttt{struct\ page}（不管其存在与否）的虚拟地址是固定且连续的，因为分配虚拟地址并不会有实际的开销，反而可以方便进行索引。SPARSEMEM-VMEMMAP
内存模型可以如同平坦内存模型一样快速进行页帧编号和\texttt{struct\ page*}的切换，使得稀疏内存模型可以完全替代平坦内存模型和非连续内存模型。

2021 年提交补丁\href{https://lwn.net/Articles/858333}{Remove
DISCINTIGMEM memory model} 彻底移除了 DISCONFITMEM 内存模型，SPARSEMEM
内存模型成为 Linux 内核的默认内存模型。


\section{本论文使用的内存模型及其实现}

教学操作系统没有对内存热插拔的需求，非连续内存模型支持 NUMA
架构，并且复杂度适中，因此论文描述的系统实现了非连续内存模型。

\begin{figure}
\centering
\includesvg[width=500pt]{images/lzuos-memory-model.drawio.svg}
\caption{本论文实现的内存模型的逻辑结构}
\end{figure}

目前本论文描述的操作系统是还不支持对称多处理器，因此将系统视为只有一个节点和一个
CPU 的 NUMA
架构机器，该节点并不包含整个物理地址空间，仅仅包含系统中的物理内存。节点内的物理内存被进一步划分为多个区域\texttt{struct\ zone}，每个区域都有自己独立的内存管理结构\texttt{struct\ page*mem\_map}。上图展示了本论文实现的内存模型的逻辑结构。在实际的实现中，节点存储了整个节点内存区域的\texttt{struct\ page\ mem\_map{[}{]}}，每个\texttt{zone}中的\texttt{struct\ page\ mem\_map{[}{]}}只是指向节点的\texttt{mem\_map{[}{]}}中对应区域的\texttt{struct\ page*}指针。物理结构如下图所示。

\begin{figure}
\centering
\includesvg[width=500pt]{images/lzuos-memory-model-implementation.drawio.svg}
\caption{本论文实现的内存模型的物理结构}
\end{figure}

划分\texttt{zone}的依据是系统中不同区域的物理内存特性不同。例如，x86\_64
架构中只有低 32MB 的物理内存能够用于 DMA，因此用于 DMA
的内存必须从该\texttt{zone}分配；在内存热插拔场景下，内核的代码数据要放在不会被下线（拔出）的内存区域中，而某些用户数据可以放在可能被下线的内存区域中，因此还应该将可能下线的内存区域划分为一个\texttt{zone}。本论文的实现基于
RISCV 架构，并且运行在 qemu 模拟器中，通过 virtio 协议操作外设，DMA
不对物理地址做限制，但仍然区分了常规内存分配请求和用于 DMA
的内存分配请求，将节点的内存划分为\texttt{ZONE\_DMA}和\texttt{ZONE\_NORMAL}区域，也许网络协议栈和驱动程序会有这方面的需求。

尽管目前不支持对称多处理器，无法完整地支持硬件 NUMA
架构，但已经在软件层面完成了绝大多数工作，未来支持对称多处理器后，只需要进行
NUMA 配置探测，初始化多个节点即可。

本章所有功能实现代码托管在 GitHub 仓库。

仓库链接：\href{https://github.com/RvOSLab/lzu_oslab}{https://github.com/RvOSLab/lzu\_oslab}。

\chapter{页分配器}

\emph{页分配器}（\emph{page
allocator}）指按页管理系统内存的分配与释放的内存分配器，是操作系统最基础的组件。本论文的页分配器使用的主要算法是\emph{伙伴系统}（Binary
Buddy Allocator），该算法由 Knowlton{\cite{knowlton1965fast}} 提出并由 Knuth
{\cite{knuth1968art}} 进一步描述。与其他内存分配算法相比，伙伴系统有以下优点：

\begin{itemize}
\item
  算法简单：伙伴系统算法相对简单，易于实现。该算法不需要维护大量的元数据信息，所以比其他算法更加轻量化和高效。
\item
  分配和释放高效：伙伴系统在分配和释放内存时，只需简单的合并或拆分节点即可。
\item
  在理想负载下没有内碎片：伙伴系统将内存划分为一系列大小相等的块，并且仅能分配\(2^N\)页，对于\(2^N\)页的内存分配请求不存在内存碎片的问题。
\end{itemize}

伙伴系统一种将普通的二次幂分配器与空闲缓冲区合并 {[}Vah96{]}
相结合的分配方案，其背后的基本概念非常简单。
内存被划分成内存块，每个内存块由\(2^N\)个连续的物理页组成，其中\texttt{N}
称为该内存块的\emph{阶数}（\emph{order}）。分配时如果没有所需大小的内存块，则将一个更大的内存块平分成两半，平分出来的内存块彼此互为伙伴。其中一个用于分配，另一半空闲。内存块会根据需要连续平分，直到平分出所需大小的内存块。
稍后释放一个内存块时，将检查它的伙伴的分配状态，如果它是空闲的，则将两者合并。合并得到新的空闲内存块后，再去检测它的伙伴，继续上述合并过程，知道无法合并。

伙伴系统有多种实现方式，如基于二叉树的实现通过一个数组形式的完全二叉树管理内存，二叉树的节点标记相应内存块的使用状态，高层节点对应阶数大的块，低层节点对应阶数小的块，在分配和释放中通过这些节点的标记属性来进行块的分离合并。

\begin{figure}
\centering
\includegraphics[width=400pt]{images/buddy-system-binary-tree-implementation.jpg}
\caption{伙伴系统的二叉树实现}
\end{figure}


\section{物理页管理}

本系统为每个节点中的页帧都保留一个对应的\texttt{struct\ page}结构体，该结构体定义如表\ref{table:struct-page-definition}所示。

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
成员名称 & 数据类型 & 描述 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{flags} & \texttt{uint32\_t} & 物理页状态 \\
\texttt{count} & \texttt{int32\_t} & 物理页使用者数量 \\
\texttt{private} & \texttt{uint64\_t} & 存储私有数据 \\
\texttt{lru} & \texttt{struct\ linked\_list\_node} & 链表节点 \\
\caption{struct page 定义}
\label{table:struct-page-definition}
\end{longtable}

系统要为每个跟踪的物理页保留一个\texttt{struct\ page}结构体，因此\texttt{struct\ page}的大小必须要足够小，以便保留足够多的可用内存。因此\texttt{struct\ page}是整个系统中最为复杂的结构体，其复杂性在于每个\texttt{struct\ page}的字段都是精心编码的，在不同的场景下有不同的含义。

\texttt{flags}保存物理页状态，包括物理页是否是被保留的（不可用于分配）、是否用于
slab 分配器、是否空闲等等。此外，\texttt{flags}的高 6
个字节还存储该页所属的\texttt{node} id 和\texttt{zone} id，因此只有 26
比特用于跟踪页面状态。

\begin{figure}
\centering
\includesvg{images/page-flags.svg}
\caption{struct page 标志}
\end{figure}

其余字段根据\texttt{flags}标示的状态有不同的含义。\texttt{private}状态存储页面相关的私有数据，目前仅用于页面被伙伴系统管理时标示内存块的阶数。\texttt{lru}是
\emph{Last Recently Used}
的简称，是一个链表节点，可用于页面置换；当页面空闲且被伙伴系统管理时，它用于标示下一块空闲内存块；当页面被分配给
slab 分配器时，\texttt{lru}用于记录该页面所属的 slab 和 cache。

本系统通过分离链表实现伙伴系统。如图\ref{fig:page-allocator-structure}所示，维护一个指向不同阶数的空闲内存块链表的数组\texttt{struct\ free\_area\ free\_zreas{[}MAX\_GFP\_ORDER+1{]}}。
数组的第 0 个元素指向一个阶数为 0 的空闲内存块链表，第 N
个元素指向阶数为 N
的空闲内存块链表，\texttt{MAX\_GFP\_ORDER}是伙伴系统支持的最大阶数。每个\texttt{zone}
独立管理自己的内存，因此每个\texttt{zone}有自己独立的伙伴系统。

\begin{figure}
\centering
\includesvg{images/free-area.drawio.svg}
\label{fig:page-allocator-structure}
\caption{页分配器物理结构}
\end{figure}

\texttt{struct\ free\_area}字段如表\ref{table:struct-free-area-definition}所示。

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
字段名称 & 数据类型 & 描述 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{free\_list} & \texttt{struct\ linked\_list\_node} &
空闲列表头指针 \\
\texttt{nr\_free} & \texttt{int32\_t} & 空闲列表包含的空闲块数量 \\
\label{table:struct-free-area-definition}
\caption{\texttt{struct\ free\_area} 定义}
\end{longtable}

空闲块的第一页称为\emph{首页}（\emph{first
page}），其余页面均称为\emph{尾页}（\emph{tail
page}），首页的\texttt{struct\ page}代表整个空闲块的状态，\texttt{free\_list}空闲列表指向第一个空闲块首页的\texttt{lru}链表节点，空闲块通过首页\texttt{struct\ page}的\texttt{lru}链表节点链接起来。


\section{接口设计}

伙伴系统提供的 API 见表\ref{table:slab-allocator-api}。

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.7596}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.2404}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
函数声明
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
介绍
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{struct\ page\ *alloc\_pages\_node(struct\ node\ *node,\ uint32\_t\ order,\ gfp\_t\ flags)}
& 从节点\texttt{node}分配\texttt{order}阶内存块 \\
\texttt{struct\ page\ *alloc\_pages(uint32\_t\ order,\ gfp\_t\ flags)} &
从当前 CPU 所属节点分配\texttt{order}阶内存块 \\
\texttt{struct\ page\ *alloc\_page(gfp\_t\ flags)} & 从当前 CPU
所属节点分配一页内存 \\
\texttt{void\ free\_pages(struct\ page\ *page,\ uint32\_t\ order)} &
释放首页为\texttt{page}的\texttt{order}阶内存块 \\
\texttt{void\ free\_pages(struct\ page\ *page)} &
释放物理页\texttt{page} \\
\label{table:slab-allocator-api}
\caption{slab 分配器 API}
\end{longtable}

其中\texttt{gfp\_t}标志决定页分配器如何分配内存。\texttt{gfp\_t}标志可以分为以下三大类：

\begin{itemize}
\tightlist
\item
  调整行为：系统内存不足时，请求内存分配的进程可以休眠，等获取了内存的进程释放内存后再重试。但在中断处理例程中，进程不得休眠，否则迟迟不结束的中断处理例程会导致这期间的所有同类型中断丢失，严重影响系统吞吐量。
\end{itemize}

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
gfp\_t 标志 & 介绍 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{\_\_GFP\_WAIT} & 允许内存分配导致进程休眠 \\
\texttt{\_\_GFP\_NORETRY} & 内存分配失败后不再重试 \\
\texttt{\_\_GFP\_NOFAIL} & 内存分配失败后无限重试 \\
\caption{用于调整行为的\texttt{gfp\_t}标志}
\label{table:gfp-action-modifier}
\end{longtable}

\begin{itemize}
\tightlist
\item
  调整内存分配区域：内存被划分为多个\texttt{node}节点，\texttt{node}节点的内存又被划分为多个\texttt{zone}，用户可以要求页分配器从特定类型的\texttt{zone}分配内存。
\end{itemize}

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
gfp\_t 标志 & 介绍 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{\_\_\_GFP\_NORMAL} & 从 \texttt{ZONE\_NORMAL}区分配内存 \\
\texttt{\_\_\_GFP\_DMA} & 从 \texttt{ZONE\_DMA}区分配内存 \\
\caption{用于调整内存分配区域的\texttt{gfp\_t}标志}
\label{table:gfp-zone-modifier}
\end{longtable}

\begin{itemize}
\tightlist
\item
  内存分配类型：前两类\texttt{gfp\_t}标志是最底层的\texttt{gfp\_t}标志，需要用户了解页分配器的内部实现才能正确使用。页分配器根据内存分配类型组合前两类\texttt{gfp\_t}提供给用户使用，用户应当优先使用描述内存分配类型的\texttt{gfp\_t}标志。
\end{itemize}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.2400}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.7600}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
gfp\_t 标志
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
介绍
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{GFP\_KERNEL} &
内核常规的分配请求。从\texttt{ZONE\_DMA}区分配内存，且可能阻塞进程。 \\
\texttt{GFP\_DMA} & 分配用于 DMA 的内存。 \\
\texttt{GFP\_ATOMIC} &
高优先级的内存分配请求，在中断处理函数和持有锁的关键区使用，不导致进程睡眠。 \\
\label{table:gfp-type-modifier}
\caption{用于指定内存分配类型的\texttt{gfp\_t}标志}
\end{longtable}

系统通过\texttt{struct\ page}
管理物理页，物理页的分配和释放都通过修改\texttt{struct\ page}
结构体实现，因此伙伴分配器的接口通过内存块首页的\texttt{struct\ page}指针来定位内存块。但从用户视角看，用户期望分配内存返回该内存块的起始虚拟地址，释放时传递该地址给伙伴系统的内存释放
API。因此，在伙伴系统提供的这些 API
之上，提供了用起始地址定位内存块的用户
API，其他子系统应该优先使用这些用户 API。用户 API 见表\ref。

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.6164}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.3836}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
函数声明
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
介绍
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{void\ *get\_free\_pages(uint32\_t\ order)} &
分配\texttt{order}阶内存块，返回其起始虚拟地址。 \\
\texttt{void\ *get\_free\_page()} &
分配一页内存，返回其起始虚拟地址。 \\
\texttt{void\ free\_pages(void\ *addr,\ uint32\_t\ order)} &
释放起始虚拟地址为\texttt{addr}的\texttt{order}阶内存块。 \\
\texttt{void\ free\_page(void\ *addr)} &
释放起始虚拟地址为\texttt{addr}的页。 \\
\label{table:slab-allocator-user-api}
\caption{slab 分配器用户 API}
\end{longtable}


\section{回退列表}

在本系统的非连续内存模型下，不同\texttt{zone}区域之间并非是互斥的，而是具有一定的优先级关系，优先级关系如下：

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  优先从本地节点分配：本地节点的\texttt{zone}优先于其他节点的\texttt{zone}。
\item
  优先从目标类型的\texttt{zone}分配：\texttt{ZONE\_NORMAL}区域优先于\texttt{ZONE\_DMA}区域。
\end{enumerate}

系统中所有的\texttt{zone}构成了一个\emph{回退列表}（\emph{fallback
list}），回退列表的第一个元素是目标\texttt{zone}，从目标\texttt{zone}分配内存失败，则回退到下一个优先级更低的\texttt{zone}分配内存，尝试完整个回退列表的\texttt{zone}后仍然无法成功分配内存才算失败。例如，用户请求从\texttt{ZONE\_NORMAL}区分配
8
页内存，页分配器会优先尝试本地\texttt{ZONE\_NORMAL}区，失败则尝试本地\texttt{ZONE\_DMA}区，仍然失败再去其他节点重复以上过程。

每个\texttt{struct\ node}节点都有自己的回退列表，在系统启动过程初始化。节点有多个\texttt{zone}，因此节点有多个回退列表，见表\ref{table:node-zone-lists}。

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1188}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4257}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4554}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
成员名
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
数据类型
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
介绍
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{zone\_lists} &
\texttt{struct\ zone\_list\ zone\_lists{[}MAX\_NR\_ZONES{]}} &
\texttt{struct\ node}节点为自己的所有\texttt{struct\ zone}都设置了对应的回退列表。 \\
\label{table:node-zone-lists}
\caption{\texttt{node}节点的回退列表}
\end{longtable}

回退列表\texttt{struct\ zone\_list}定义j见表\ref{table:zone-list-definition}：

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.0693}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4752}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4554}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
成员名
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
数据类型
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
介绍
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{zones} &
\texttt{struct\ zone*\ {[}MAX\_NR\_NODES\ *\ MAX\_NR\_ZONES\ +\ 1{]}} &
\texttt{struct\ zone*}指针数组，数组元素指向对应\texttt{zone}，数组以\texttt{NULL}结尾。 \\
\label{table:zone-list-definition}
\caption{\texttt{struct\ zone}定义}
\end{longtable}


\section{页分配与释放}

页分配算法流程如下：

\begin{figure}
\centering
\includesvg{images/page-alloc-flowchart.drawio.svg}
\caption{页分配算法流程图}
\end{figure}

释放内存块流程如下：

\begin{figure}
\centering
\includesvg{images/page-deallocate-flowchart.drawio.svg}
\caption{页释放算法流程图}
\end{figure}


\section{对抗内存碎片}

伙伴系统的内存碎片来源如下：

\begin{itemize}
\item
  内碎片：内存分配请求不是\(2^N\)页，极端情况下内碎片可以到达 50\%。
\item
  外碎片：释放内存块时，合并路径上某个伙伴已分配导致无法合并伙伴。
\end{itemize}

伙伴系统的外碎片较少，可以忽略不计。外部碎片是由于未分配的连续内存区域太小，以至于不能满足所需要的内存分配请求。伙伴系统在分配小块连续内存时不会从大块连续空间中截取一小段，从而保证了大块内存的连续和完整，因此伙伴系统不存在严重的外碎片问题，或者说页分配器使用伙伴系统克服了外碎片。

页分配器克服内碎片的方式是避免非\(2^N\)页的内存分配。系统将页分配器作为最底层的内存分配器，只服务\(2^N\)页的内存分配请求。非\(2^N\)页的内存分配请求由上层内存分配器服务，页分配器作为最底层的内存分配器为其提供内存。因此，本系统的页分配器内碎片为
0。

伙伴系统只适合分配\(2^N\)页大小的内存，因此包括 FreeBSD、Linux 和
jemalloc
等采用伙伴系统算法的系统将其用于实现页分配器，再其上再建立更复杂的内存分配器服务非\(2^N\)大小的内存分配请求。

本系统在页分配器上实现 slab 分配器，用于满足非\(2^N\)页大小的内存请求。

本章所有功能实现代码托管在 GitHub 仓库。

仓库链接：\href{https://github.com/RvOSLab/lzu_oslab}{https://github.com/RvOSLab/lzu\_oslab}。

\chapter{slab 分配器}

slab 分配器起源自 SunOS 5.4{\cite{bonwick1994slab}}
内核，其设计思路是通过缓存对象在多次使用间的结构状态来降低分配复杂对象的成本。这一方案背后的合理性在于，用户的内存分配请求实际上不是为了内存自身，而是为了构造对象；用户释放内存后，往往又要分配内存来构造统一类型的对象。并且，在许多场景下，对象构造和析构的成本并不低于分配和释放内存的成本。因此
slab
分配器将对象缓存起来，从而维持了对象的结构状态，所以对象不需要每次使用前构造，使用后析构，进而提升了系统性能。使用
slab
分配器后，对象的声明周期由``构造-使用-析构''变成``构造-使用-使用-使用-析构''。slab
分配器的对象缓存策略不仅对有状态内存（对象）有用，对无状态内存（数据缓冲区等）页也同样高效。

除了缓存对象外，slab 分配器还有以下优点：

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  缓存利用率高：slab 分配器通过\emph{slab 着色}（\emph{slab
  coloring})提高了系统总体的缓存利用率和总线平衡程度。
\item
  内存碎片小且可调节：slab 分配器无外碎片，且对于容量为 N 个对象的 slab
  分配器，内碎片比例比超过 1/N。
\item
  客户端驱动的分离存储：传统的分离存储需要在编译期硬编码多个内存块大小固定的空闲列表，而
  slab 分配器的对象大小在运行时由用户指定。
\item
  缓存之间无共享状态：可以轻易实现任意多个缓存。
\item
  前后端分离：用户从 cache 中分配释放内存视作前端，slab
  分配器从底层内存分配器获取/释放内存视作后端。
\end{enumerate}

\begin{figure}
\centering
\includesvg{images/slab-allocator-architecture.drawio.svg}
\caption{slab 分配器架构}
\end{figure}


\section{接口设计}

slab 分配器提供了以下四个接口如下：

\begin{lstlisting}[language = c]
// 创建名为 name，对象大小为 size，对齐到 align，构造函数/析构函数分别为 ctor/dtor 的缓存。
struct kmem_cache *
kmem_cache_create(const char name, uint64_t size, uint64_t align,
        uint32_t flags,
        void (ctor)(void *, struct kmem_cache *, uint32_t),
        void (dtor)(void *, struct kmem_cache *, uint32_t));
// 从 cachep 指向的缓存中分配一个对象
void* kmem_cache_alloc(struct kmem_cache *cachep, gfp_t flags);
// 释放 objp 指向的对象，不释放底层内存。
void kmem_cache_free(struct kmem_cache *cachep, void *objp);
// 删除缓存中的空闲的 slab
void kmem_cache_reap(struct kmem_cache *cachep);
// 销毁缓存并释放其内存
void kmem_cache_destroy(struct kmem_cache *cachep);

\end{lstlisting}

\texttt{kem\_cache\_create()}创建 cache
时，指定了对象的构造函数和析构函数，slab
分配器在为该对象分配内存时调用对象的构造函数，在释放该对象的内存时调用对象的析构函数。对象的构造函数和析构函数原型相同，完整的声明见表\ref{table:obj-ctor-dtor}。

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.4151}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5849}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
函数原型
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
介绍
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{void\ *ctor(void\ *objp,\ struct\ kmem\_cache\ *cachep,\ uint32\_t\ flag)}
&
\texttt{objp}是对象起始虚拟地址，\texttt{cachep}指向对象所属的缓存，lag
告知构造函数其运行环境，用于调整构造函数的行为。返回值为\texttt{void*}，意味着构造函数可以返回任意类型的数据。 \\
\texttt{void\ *dtor(void\ *objp,\ struct\ kmem\_cache\ *cachep,\ uint32\_t\ flag)}
& 同上 \\
\label{table:zone-list-definition}
\caption{构造函数/析构函数的声明}
\end{longtable}

传递给构造函数和析构函数的\texttt{flag}见图\ref{table:ctor-dtor-flags}。

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
标志名 & 介绍 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{CTOR\_ATOMIC} & 要求构造函数不得导致进程休眠 \\
\label{table:ctor-dtor-flags}
\caption{构造函数和析构函数接收的\texttt{flag}}
\end{longtable}

以分配磁盘中文件的管理结构\texttt{struct\ inode}为例，展示典型的 slab
分配器使用方式。

\begin{lstlisting}[language = c]
#include <slab.h>

struct inode;
extern void init_inode(struct inode *inode, uint32_t flags);
extern void destory_inode(struct inode *inode, uint32_t flags);
extern void use(struct inode *inode);

void inode* ctor(void *objp, struct kmem_cache cachep, uint32_t flags) {
    struct inode inode = (struct inode *)objp;
    init_inode(inode);
    return NULL;
}

void inode* dtor(void *objp, struct kmem_cache cachep, uint32_t flags) {
    struct inode inode = (struct inode *)objp;
    destory_inode(inode);
    return NULL;
}

// ...
struct kmem_cache *inode_cachep =
kmem_cache_create("inode_cache", sizeof(struct inode), 0, 0, inode_ctor);
struct inode *inode = kmem_cache_alloc(inode_cachep, GFP_KERNEL);
use(inode);
kmem_cache_free(inode_cachep, inode);
kmem_cache_destroy(inode_cachep);
// ...

\end{lstlisting}


\section{数据结构}

slab 分配器的结构如下：

\begin{figure}
\centering
\includesvg[width=500pt]{images/kmem_cache-physical-structure.drawio.svg}
\caption{slab 分配器物理结构}
\end{figure}

\texttt{struct\ kmem\_cache}描述一个
cache，定义较为复杂，下文会在具体算法流程中详细介绍。\texttt{struct\ kmem\_cache}字段见表\ref{table:kmem_cache-definition}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1197}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4615}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4188}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
字段名称
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
类型
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
介绍
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{flags} & \texttt{uint32\_t} & \texttt{kmem\_cache\_create}
标志，决定 cache 创建 slab 和分配对象的行为。 \\
\texttt{obj\_size} & \texttt{uint32\_t} & 对齐后的对象大小 \\
\texttt{obj\_num} & \texttt{uint32\_t} & slab 能容纳的最大对象数量 \\
\texttt{slab\_size} & \texttt{uint32\_t} & slabmgt 占用的字节数 \\
\texttt{free\_limit} & \texttt{uint32\_t} & cache
能容纳的空闲对象的最大数量 \\
\texttt{slabp\_cache} & \texttt{struct\ kmem\_cache\ *} & 分配 slab
管理结构的 kmem\_cache \\
\texttt{objp\_cache} & \texttt{struct\ objp\_cache*} & 缓存本 cache
中空闲对象指针 \\
\texttt{gfpflags} & \texttt{gfp\_t} & 底层页分配器标志 \\
\texttt{order} & \texttt{uint32\_t} & 底层页分配器分配内存的阶数 \\
\texttt{slab\_full} & \texttt{struct\ linked\_list\_node} & 容量已满的
slab 所处的链表 \\
\texttt{slab\_empty} & \texttt{struct\ linked\_list\_node} & 空闲 slab
所处的链表 \\
\texttt{slab\_partial} & \texttt{struct\ linked\_list\_node} &
容量未满且非空的 slab 所处的链表 \\
\texttt{free\_objs} & \texttt{uint32\_t} & cache 中的空闲对象数量 \\
\texttt{color} & \texttt{uint32\_t} & slab 着色范围 \\
\texttt{color\_off} & \texttt{uint32\_t} & slab 着色偏移 \\
\texttt{color\_next} & \texttt{uint32\_t} & 下一 slab 的颜色 \\
\texttt{ctor} &
\texttt{void\ (ctor)(void\ *,\ struct\ kmem\_cache\ *,\ uint32\_t)} &
对象构造函数 \\
\texttt{dtor} &
\texttt{void\ (dtor)(void\ *,\ struct\ kmem\_cache\ *,\ uint32\_t)} &
对象析构函数 \\
\texttt{name} & \texttt{const\ char*} & cache 名称 \\
\texttt{list} & \texttt{struct~linked\_list\_node} & cache
自身的链表节点 \\
\label{table:kmem_cache-definition}
\caption{\texttt{struct\ kmem\_cache}定义}
\end{longtable}

\texttt{struct\ slab}字段见表\ref{table:struct-slab-definition}。

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1528}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3472}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
字段名称
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
类型
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
介绍
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{color\_off} & \texttt{uint32\_t} & slab 颜色与 slab
管理对象之和，即对象到缓存区起始地址的偏移。 \\
\texttt{mem} & \texttt{void\ *} & slab 缓存区中第一个对象的地址 \\
\texttt{inuse} & \texttt{uint32\_t} & slab 中已分配对象数量 \\
\texttt{list} & \texttt{struct\ linked\_list\_node} & slab 的链表节点 \\
\texttt{freelist} & \texttt{kmem\_bufctl\_t} & 对象空闲列表 \\
\label{table:struct-slab-definition}
\caption{\texttt{struct\ slab}定义}
\end{longtable}

\texttt{struct\ objp\_cache}定义见表\ref{table:struct-objp-cache-definition}。

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2031}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1406}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.6562}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
字段名称
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
类型
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
介绍
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{capacity} & \texttt{int32\_t} & 数组容量 \\
\texttt{avail} & \texttt{int32\_t} & 已使用元素数量 \\
\texttt{batch\_count} & \texttt{int32\_t} &
批处理数量，每次修改\texttt{objp\_cache}均修改\texttt{batch\_count}个元素。 \\
\texttt{touched} & \texttt{int32\_t} & \texttt{objp\_cache}是否被访问 \\
\label{table:struct-objp-cache-definition}
\caption{\texttt{struct\ objp\_cache}定义}
\end{longtable}


\section{对象管理}

slab 分配器以 slab 为内存管理基本单位。slab 包含一段连续的物理内存，称为
slab buffer。slab 缓存区由页分配器分配，因此其大小为 \(2^N\) 页。slab
缓存区布局如下：

\begin{figure}
\centering
\includesvg[width=500pt]{images/on-slab-and-off-slab.drawio.svg}
\caption{slab 管理结构}
\end{figure}

将大小小于 1/8 页的对象视作小对象。对于小对象，slab
分配器从页分配器分配一页作为 slab buffer，把 slab 管理结构 slabmgt
和对象都放置在 slab buffer 中。这种将 slab 管理结构 slabmgt
和被管理的对象放在一起的情形，称为 on-slab。

将大小大于 1/8 页的对象视作大对象。off-slab 的设计对大对象不利，由于
slabmgt 页放置在 slab buffer 中，对象无法完全占用 slab buffer
的空间，会导致严重的内碎片。例如对于 4K 大小的 slab
buffer，由于部分空间用于存储 slabmgt，一个 slab 只能存储一个 2K
大小的对象，剩下近 2K 内存均被浪费。因此，对于大对象，必须将 slabmgt
和对象分离存储。这种分离存储的情形称为 off-slab。off-slab 的 slabmgt
是小对象，适合通过 slab 分配器分配。因此 slab 分配器会为 cache 保留分配
slabmgt 的 cache，cache 的\texttt{slab\_cachep}字段指向为其分配 slabmgt
的 cache。

slabmgt
包括\texttt{struct\ slab}和对象空闲列表。对象空闲列表实现为基于数组的链表，空闲列表的元素称为
bufctl（缓存区控制符），其类型为\texttt{bufctl\_t}，在本系统中是\texttt{uint16\_t}的别名。每个
bufctl 均代表一个对象，空闲列表第 N 个 bufctl 对应缓存区中第 N
个对象。bufctl
的值是空闲列表中它的后继的下标，空闲列表末尾元素的值为\texttt{BUFCTL\_END}，标示空闲列表终止，空闲列表的表头是\texttt{struct\ slab}的\texttt{freelist}字段，它指向空闲列表第一个元素。

空闲列表的结构如下图所示，虚线箭头表示 bufctl
和对象的对应关系，实线箭头表示空闲列表元素的链接关系。空白块表示该对象未分配，灰色块表示该对象已分配。

\begin{figure}
\centering
\includesvg[width=500pt]{images/freelist-management.drawio.svg}
\caption{slab 分配器对象空闲列表}
\end{figure}

通常 slab buffer 存放 slabmgt 和对象后，还会剩下一段内存，slab 分配器把
slabmgt 和对象放在 slab buffer 的最后面，将剩下的这段内存放在 buffer
最前面，这段内存用于 slab 着色，\ref{sec:hw-cache-efficiency}“硬件缓存利用率”详细介绍。

和其他分配算法相同，分配释放对象最终体现为对空闲列表的操作。分配对象时从空闲列表摘取第一个
bufctl，释放对象时将 bufctl 插入到空闲列表头部中。

下图展示对象分配释放对空闲列表的修改。初始时，slab 空闲，空闲列表为
freelist-\textgreater0-\textgreater1-\textgreater2-\textgreater3；分配一个对象后空闲列表为
freelist-\textgreater1-\textgreater2-\textgreater3；再次分配后空闲列表为
freelist-\textgreater2-\textgreater3；释放对象 0 后，空闲列表为
freelist-\textgreater0-\textgreater2-\textgreater3。

\begin{figure}
\centering
\includesvg[width=500pt]{images/alloc-obj-freelist-management.drawio.svg}
\caption{slab 分配器对象空闲链表分配释放过程}
\end{figure}


\section{对象分配与释放}

slabmgt
记录了对象的分配情况，而\texttt{objp\_cache}缓存了可分配对象的指针。用户请求分配时
slab allocator
直接从\texttt{objp\_cache}摘取一个对象指针并返回即可，从而减少了修改底层
slab 空闲列表的次数，提高了内存分配性能。

\begin{figure}
\centering
\includesvg{images/alloc-obj-simple-flowchart.drawio.svg}
\caption{objp\_cache 缓存}
\end{figure}

具体流程如下：

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  如果\texttt{objp\_cache}非空，则直接从\texttt{objp\_cache}分配即可。
\item
  如果\texttt{objp\_cache}已空，则尝试从 slab 分配。优先从非空 slab
  分配对象，没有非空对象才从空闲 slab
  分配对象。如果连空闲对象都没有，这时 slab
  分配器需要从底层页分配器分配内存，新建一个 slab 并从中分配。
\item
  为了减少底层 slab
  分配器的分配次数，每次从底层分配器分配\texttt{objp\_cache-\textgreater{}batch\_count}个对象并将其指针压入\texttt{objp\_cache}末尾；如果\texttt{objp\_cache-\textgreater{}touched}为
  0，说明该 cache 的内存分配并不活跃，因此只向底层 slab
  分配器请求更少的\texttt{INACTIVE\_BATCH\_COUNT}个对象。
\item
  从\texttt{objp\_cache}末尾取对象指针并返回。
\end{enumerate}

对象分配流程图如下：

\begin{figure}
\centering
\includegraphics[width=500pt]{images/alloc-obj-flow-chart.drawio-cropped.pdf}
\caption{slab 分配器对象分配流程图}
\end{figure}

cache 中的每个 slab 都是相同的，对象大小、对象数量、slab
占用的内存等配置由\texttt{kmem\_cache\_create()}设置，分配 slab
只需向底层页分配器请求内存，初始化
slabmgt，调用构造函数构造对象，并将新创建的 slab
假如到\texttt{slab\_partial}链表即可。

释放对象是分配对象的逆过程，具体流程如下：

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  修改 slab 的空闲列表，释放对象。
\item
  根据 slab
  中空闲对象的数量，将其加入到\texttt{slab\_partial}或\texttt{slab\_empty}链表。
\item
  若 slab 已空，且 cache
  中的空闲对象数量超过\texttt{free\_limit}，则销毁该
  slab（调用析构函数，释放 slab 内存）。
\end{enumerate}


\section{调试功能}

据论文 xx 描述，SunOS 5.4 利用 slab 分配器实现了以下功能：

\begin{itemize}
\item
  内存审计：slab
  分配器保存它的所有内存分配记录，包括时间戳、栈轨迹、调用者所在线程等。此功能用于发现内存错误后调试。
\item
  被释放内存块地址验证：slab
  分配器记录所有合法的大内存块地址，\texttt{kmem\_cache\_free()}时验证被释放的内存块是否是先前分配的内存块。此功能确保\texttt{kmem\_create\_free()}释放的是合法地址上的内存。
\item
  实现于用户态的内核内存泄露检测器：slab
  分配器的内存审计模块记录了分配内存的时间戳，并通过设备文件 dev/kmem
  将此数据暴露给用户态。用户态内存泄露检测器周期性的扫描
  /dev/kmem，如果发现某块内存在很久之前分配，但目前仍未被释放，则这块内存很可能发生了内存泄露。这种方案只是报告可能发生的内存泄露，无法确定是否真的发生内存泄露。
\item
  缓存区溢出检测：在对象前后分别增加一小块区域，这块区域称为\emph{红区}（\emph{redzone}），通过检测红区判断是否发生缓冲区溢出。后文详细介绍红区检测算法。
\item
  use-after-free 检测：SunOS 5.4
  通过标记对象实现在分配对象时检测到该对象上的
  use-after-free，后文详细介绍此算法。还可以通过两个 slab
  分配器的两个拓展运行模式立即检测到 slab 区域和对象上的
  use-after-free。SunOS 5.4
  有一个\emph{同步取消映射模式}（\emph{synchronous-unmapping
  mode}），在此模式下销毁 slab
  不仅释放内存，还取消对于内存区域的页表映射，下次访问该 slab
  内存区域将导致页错误异常，从而实现 use-after-free
  的立即检测。此外，SunOS 5.4 还同步取消映射模式用于 slab
  对象，每个对象都占用一页，以浪费物理内存为代价实现 slab 对象上的
  use-after-free 立即检测。
\end{itemize}

本系统实现了基于红区检测实现 slab 对象缓冲区越界检测，基于标记对象实现
slab 对象上的 use-after-free 检测。在对象两侧分别设置 64 字节的
redzone，redzone 算在 slab 对象中。增加调试功能后，slab 对象的布局如下：

\begin{figure}
\centering
\includesvg{images/slab-debug-layout.drawio.svg}
\caption{slab 对象布局}
\end{figure}

将 slab
对象的状态分为活跃状态与非活跃状态，对象在分配后释放前的时间视作处于活跃状态，分配前或释放后视作处于非活跃状态。这两个状态下红区和实际的
slab 对象必须为以下值，否则视作发生了内存安全 bug。

\begin{figure}
\centering
\includesvg{images/slab-debug-status.drawio.svg}
\caption{slab 对象调试标记}
\end{figure}

以 slab 对象分配释放的整个声明周期为例说明调试功能使用的算法：

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  创建 slab 时，设置调试标记。redzone
  写为\texttt{REDZONE\_INACTIVE}，对象写为\texttt{OBJ\_INUSE}。
\item
  \texttt{kmem\_cache\_alloc()}分配对象后返回前，检测调试标记。如果
  redzone
  值不等于\texttt{REDZONE\_INACTIVE}，则说明发生缓冲区溢出；如果对象值不等于\texttt{OBJ\_INUSE}，说明对象上发生
  use-after-free。对象被分配后使用过程中可能发生缓存区越界，因此 redzone
  写为\texttt{REDZONE\_ACTIVE}，表示 redzone 生效。为了检测
  use-after-free，对象被写成\texttt{OBJ\_FREE}或\texttt{OBJ\_INUSE}，因此\texttt{kmem\_cache\_alloc()}返回前还要调用构造函数重新初始化对象。
\item
  \texttt{kmem\_cache\_free()}释放对象时，检测并设置调试标记。redzone
  值不等于\texttt{REDZONE\_ACTIVE}，说明发生缓存区越界（应为\texttt{REDZONE\_ACTIVE}的值被修改）或对象上发生
  double-free（释放后的对象其 redzone
  为\texttt{REDZONE\_INACTIVE}）；对象值不用检查，因为此时对象中是用户数据。将
  redzone 写为\texttt{REDZONE\_INACTIVE}，对象值写成\texttt{OBJ\_FREE}。
\item
  销毁 slab 时，检测并设置调试标记。redzone
  不等于\texttt{REDZONE\_INACTIVE}说明发生缓冲区越界；对象值不等于\texttt{OBJ\_FREE}说明发生
  use-after-free。
\end{enumerate}


\section{硬件缓存利用率}\label{sec:hw-cache-efficiency}

现代硬件的内存访问性能严重依赖于硬件缓存，因此内存分配器必须将硬件缓存纳入考虑。通常，对于内存分配器，主要有以下两类因素影响硬件缓存性能：

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  对象地址的分布。内存分配器分配的对象应当尽可能的分布在硬件缓存中，尽可能降低缓存不命中的概率。
\item
  内存分配器自身的代码路径。内存分配器自身的代码要尽量少地影响硬件缓存。
\end{enumerate}

slab
分配器实际上是一个分离存储的内存分配器，为大小不同的对象单独维护一个空闲列表（即本系统中的\texttt{struct\ kmem\_cache}）。分配对象可以直接查找到空闲对象并分配，释放对象主要是修改指针，因此
slab 分配器对硬件缓存的影响很小。并且，对于小对象，slabmgt 和 slab
对象等相关的数据均放在一页中，不仅空间局部性良好，TLB 未命中率也比较低。

slba
分配器主要需要考虑对象地址的分布，本系统使用以下措施提高硬件缓存性能：

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  对象默认对齐到缓存行大小：对齐到缓存行大小可以确保小于缓存行大小的对象存放在不同缓存行，从而避免\emph{伪共享}（\emph{false
  share}）。
\item
  \texttt{SLAB\_HW\_CACHE\_ALIGN}标志尝试将小于缓存行大小的对象存放于同一缓存行：如果用户明确清除
  slab
  中的数据是具有相关性的，可以通过此标志使用尽可能小的对齐，提高相邻对象被放置到同一缓存行的概率。
\item
  slab 着色：为每个 slab 设置的起始偏移（称为该 slab
  的颜色），以提高不同 slab
  中的对象被放置到硬件缓存不同的组的概率，从而降低组相联不命中率。
\end{enumerate}


\section{kmalloc}

kmalloc 即 kernel malloc 的缩写，指内核态的\texttt{malloc(3)}函数。slab
分配器的对象缓存策略对无状态内存也同样有效，因此\texttt{kmalloc()}只是
slab 分配器的包装函数。

本系统在系统启动时创建对象大小为 16B 到 32K 的 12
个\texttt{struct\ kmem\_cache}缓存，称为 kmalloc
缓存。\texttt{kmalloc()}分配内存时搜索对象大小最接近的 kmalloc
缓存并从中分配对象。


\section{内存碎片与内存浪费情况分析}

slab 分配器中，每种对象的缓存只能分配该对象，因此不存在外碎片问题。

slab 分配器的内碎片也比较少。slab 分配器将 slab 缓冲区根据对象大小平分为
N 等分，每一等分就是对象占用的内存，因此其内存碎片率最多不超过
1/N。这一特性也使得 slab 分配器的内碎片比例可调节，通过增大 slab
缓存区大小，可以容纳更多的对象，使得内碎片率下降。本系统的实现认为内碎片比例不超过
1/8 是可接受的，当内碎片比例超过 1/8 时，尝试增大 slab
缓存区来降低内碎片比例。

slab
分配器没有严重的内存碎片问题，但存在一定的内存浪费问题。试想用户创建了一个
slab 可容纳 100 个对象的 cache，但却只分配了 2 个对象，剩下 98
个对象的内存都是浪费的。本系统的解决方案如下：

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  slab 缓存区大小设置为内存碎片比例小于 1/8 的最小大小。当 slab
  内存不够用时，再创建新的
  slab。此外，内核分配的对象以小对象为主，因此每个 slab
\item
  优先从非空的 slab 分配对象。这样可以减少 slab 的数量，降低内存浪费。
\end{enumerate}

本章所有功能实现代码托管在 GitHub 仓库。

仓库链接：\href{https://github.com/RvOSLab/lzu_oslab}{https://github.com/RvOSLab/lzu\_oslab}。

\chapter{结论与展望}

本文描述了一种教学操作系统物理内存分配器的设计与实现，其中包括内存模型的设计与实现、页分配器的设计与实现以及
slab
分配器的设计与实现。论文附录介绍物理管理模块的其他重要细节，包括物理内存探测、进程地址空间中线性映射区的建立、页分配器的建立过程、slab
分配器的初始化过程等。

本文描述的内存分配器虽然完整的实现了内存分配释放功能，但还有一些缺陷，最主要的是没有实现\emph{内存回收}（\emph{memory
reclaim}），即当空闲内存紧张时，扫描系统内存并回收可回收的内存，如空闲的
slab、磁盘缓冲区等。

尽管对于教学操作系统，本论文实现的物理内存管理模块已经足够丰富，但仍然可以畅想未来其他同学在其上添加以下功能：

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  前面所述的内存回收功能。
\item
  OOM(Out Of Memory)
  killer：当内存不足以满足内核内存分配需求时，杀死适当的用户进程并释放内存。
\item
  更多的内存安全 bug 检测和更完善的错误报告：目前，只实现了对象上的
  use-after-free 和 buffer overflow 检测，并且检测内存安全 bug
  直接终止内核运行。未来可以增加内存审计、内存泄露检测等调试功能，并在检测到内存安全错误时打印详细的错误报告，包括发生内存错误的内存区域，该内存区域创建者，最近使用者等等。
\item
  完善 NUMA
  支持：目前已经实现了非连续内存模型，但由于本系统不支持多核处理器，只能将系统视作一个单节点单处理器的
  NUMA
  机器。本系统实实现了一个专用于启动节点的支持任意大小内存分配的内存分配器，为后序
  NUMA 架构的支持奠定了基础。等到其他同学添加 SMP 支持后，可以完整实现对
  NUMA 架构的支持。
\item
  更细粒度的内存模型和内存热插拔。内存热插拔是云计算对操作系统的基本要求，实现内存热插拔后可以通过
  qemu 模拟器为同学们展示系统的动态扩缩容。
\end{enumerate}





%论文后部
\backmatter


%=======%
%引入参考文献文件
%=======%
\bibdatabase{bib/database}%bib文件名称 仅修改bib/ 后部分
\printbib
\nocite{*} %显示数据库中有的，但是正文没有引用的文献






\Thanks

一切始于 2018 年夏天的一行\texttt{Hello, World!}，这是敲下的第一行代码，也是新世界的开始。

大学第一个学期，\hyperlink{https://linuxtorvalds.com/}{Linus Torvalds} 的自传 \textit{Just for Fun: The Story of an Accidental Revolutionary}（中译本《只是为了好玩：Linux之父林纳斯自传》 和 \hyperlink{https://stallman.org/}{RMS(Richard Matthew Stallman)} 的传记 \textit{Free as in Freedom: Richard Stallman's Crusade for Free Software}（中译本《若为自由故：自由软件之父理查德·斯托曼传》）占据了我的生活，他们高昂的黑客精神深深地打动了我，促使我转专业到计算机。\hyperlink{https://www.moolenaar.net/}{Bram Moolenaar} 开发的 \hyperlink{https://www.vim.org/}{Vim} 及其后继 \hyperlink{https://neovim.io/}{Neovim}，见证了我学习工作一点一滴的进步。作为在自由软件运动和开源软件运动下成长起来的一代程序员，由衷感谢这些黑客以自己远见卓识创造了今天宏伟的软件世界，感谢互联网的盗火者冲破桎梏推动知识的自由传播。

感谢兰州大学宽松的转专业政策，让我成功转专业到计算机，不至于在自己不感兴趣的领域蹉跎青春。

感谢我的父母，他们以坚韧的毅力将我抚养成人，并始终在背后支持着我，感谢父母像蜡烛一样点燃自己为我照亮黑夜。

感谢我的女朋友孙金雨，她给我的生活增添了许多酸甜苦辣咸，感谢她陪我度过了找实习找工作的痛苦时光，在互联网寒冬中给我带来丝丝暖意。

这是论文的结束，是五年大学生涯的终点，也是我职业生涯的起点，\texttt{Hello, World!}。

\Grade %这一句才是成绩页，上面是填写

\end{sloppypar}
\end{document}
